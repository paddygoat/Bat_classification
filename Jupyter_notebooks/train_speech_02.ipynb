{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Copy these files from Tensorflow_master into home directory to be on PATH:\n",
    "\n",
    "train.py\n",
    "input_data.py\n",
    "freeze.py\n",
    "\n",
    "Copy speech_commands_v0.02.tar.gz to tmp/speech_dataset\n",
    "\n",
    "sudo jetson_clocks\n",
    "sudo nvpmodel -m 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "HelloWorld example using TensorFlow library.\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Simple hello world using TensorFlow\n",
    "\n",
    "# Create a Constant op\n",
    "# The op is added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "\n",
    "# Start tf session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the op\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "r\"\"\"Simple speech recognition to spot a limited number of keywords.\n",
    "\n",
    "This is a self-contained example script that will train a very basic audio\n",
    "recognition model in TensorFlow. It downloads the necessary training data and\n",
    "runs with reasonable defaults to train within a few hours even only using a CPU.\n",
    "For more information, please see\n",
    "https://www.tensorflow.org/tutorials/audio_recognition.\n",
    "\n",
    "It is intended as an introduction to using neural networks for audio\n",
    "recognition, and is not a full speech recognition system. For more advanced\n",
    "speech systems, I recommend looking into Kaldi. This network uses a keyword\n",
    "detection style to spot discrete words from a small vocabulary, consisting of\n",
    "\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", and \"go\".\n",
    "\n",
    "To run the training process, use:\n",
    "\n",
    "bazel run tensorflow/examples/speech_commands:train\n",
    "\n",
    "This will write out checkpoints to /tmp/speech_commands_train/, and will\n",
    "download over 1GB of open source training data, so you'll need enough free space\n",
    "and a good internet connection. The default data is a collection of thousands of\n",
    "one-second .wav files, each containing one spoken word. This data set is\n",
    "collected from https://aiyprojects.withgoogle.com/open_speech_recording, please\n",
    "consider contributing to help improve this and other models!\n",
    "\n",
    "As training progresses, it will print out its accuracy metrics, which should\n",
    "rise above 90% by the end. Once it's complete, you can run the freeze script to\n",
    "get a binary GraphDef that you can easily deploy on mobile applications.\n",
    "\n",
    "If you want to train on your own data, you'll need to create .wavs with your\n",
    "recordings, all at a consistent length, and then arrange them into subfolders\n",
    "organized by label. For example, here's a possible file structure:\n",
    "\n",
    "my_wavs >\n",
    "  up >\n",
    "    audio_0.wav\n",
    "    audio_1.wav\n",
    "  down >\n",
    "    audio_2.wav\n",
    "    audio_3.wav\n",
    "  other>\n",
    "    audio_4.wav\n",
    "    audio_5.wav\n",
    "\n",
    "You'll also need to tell the script what labels to look for, using the\n",
    "`--wanted_words` argument. In this case, 'up,down' might be what you want, and\n",
    "the audio in the 'other' folder would be used to train an 'unknown' category.\n",
    "\n",
    "To pull this all together, you'd run:\n",
    "\n",
    "bazel run tensorflow/examples/speech_commands:train -- \\\n",
    "--data_dir=my_wavs --wanted_words=up,down\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import input_data\n",
    "#from home.pi.tensorflow.tensorflow_master.tensorflow.examples.speech_commands import input_data\n",
    "#/home/pi/tensorflow/tensorflow_master/tensorflow/examples/speech_commands\n",
    "\n",
    "import models\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  # Set the verbosity based on flags (default is INFO, so we see all messages)\n",
    "  tf.compat.v1.logging.set_verbosity(FLAGS.verbosity)\n",
    "\n",
    "  # Start a new TensorFlow session.\n",
    "  sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "  # Begin by making sure we have the training data we need. If you already have\n",
    "  # training data of your own, use `--data_url= ` on the command line to avoid\n",
    "  # downloading.\n",
    "  model_settings = models.prepare_model_settings(\n",
    "      len(input_data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "      FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "      FLAGS.window_stride_ms, FLAGS.feature_bin_count, FLAGS.preprocess)\n",
    "  audio_processor = input_data.AudioProcessor(\n",
    "      FLAGS.data_url, FLAGS.data_dir,\n",
    "      FLAGS.silence_percentage, FLAGS.unknown_percentage,\n",
    "      FLAGS.wanted_words.split(','), FLAGS.validation_percentage,\n",
    "      FLAGS.testing_percentage, model_settings, FLAGS.summaries_dir)\n",
    "  fingerprint_size = model_settings['fingerprint_size']\n",
    "  label_count = model_settings['label_count']\n",
    "  time_shift_samples = int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000)\n",
    "  # Figure out the learning rates for each training phase. Since it's often\n",
    "  # effective to have high learning rates at the start of training, followed by\n",
    "  # lower levels towards the end, the number of steps and learning rates can be\n",
    "  # specified as comma-separated lists to define the rate at each stage. For\n",
    "  # example --how_many_training_steps=10000,3000 --learning_rate=0.001,0.0001\n",
    "  # will run 13,000 training loops in total, with a rate of 0.001 for the first\n",
    "  # 10,000, and 0.0001 for the final 3,000.\n",
    "  training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(',')))\n",
    "  learning_rates_list = list(map(float, FLAGS.learning_rate.split(',')))\n",
    "  if len(training_steps_list) != len(learning_rates_list):\n",
    "    raise Exception(\n",
    "        '--how_many_training_steps and --learning_rate must be equal length '\n",
    "        'lists, but are %d and %d long instead' % (len(training_steps_list),\n",
    "                                                   len(learning_rates_list)))\n",
    "\n",
    "  input_placeholder = tf.compat.v1.placeholder(\n",
    "      tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "  if FLAGS.quantize:\n",
    "    fingerprint_min, fingerprint_max = input_data.get_features_range(\n",
    "        model_settings)\n",
    "    fingerprint_input = tf.quantization.fake_quant_with_min_max_args(\n",
    "        input_placeholder, fingerprint_min, fingerprint_max)\n",
    "  else:\n",
    "    fingerprint_input = input_placeholder\n",
    "\n",
    "  logits, dropout_prob = models.create_model(\n",
    "      fingerprint_input,\n",
    "      model_settings,\n",
    "      FLAGS.model_architecture,\n",
    "      is_training=True)\n",
    "\n",
    "  # Define loss and optimizer\n",
    "  ground_truth_input = tf.compat.v1.placeholder(\n",
    "      tf.int64, [None], name='groundtruth_input')\n",
    "\n",
    "  # Optionally we can add runtime checks to spot when NaNs or other symptoms of\n",
    "  # numerical errors start occurring during training.\n",
    "  control_dependencies = []\n",
    "  if FLAGS.check_nans:\n",
    "    checks = tf.compat.v1.add_check_numerics_ops()\n",
    "    control_dependencies = [checks]\n",
    "\n",
    "  # Create the back propagation and training evaluation machinery in the graph.\n",
    "  with tf.compat.v1.name_scope('cross_entropy'):\n",
    "    cross_entropy_mean = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
    "        labels=ground_truth_input, logits=logits)\n",
    "  if FLAGS.quantize:\n",
    "    tf.contrib.quantize.create_training_graph(quant_delay=0)\n",
    "  with tf.compat.v1.name_scope('train'), tf.control_dependencies(\n",
    "      control_dependencies):\n",
    "    learning_rate_input = tf.compat.v1.placeholder(\n",
    "        tf.float32, [], name='learning_rate_input')\n",
    "    train_step = tf.compat.v1.train.GradientDescentOptimizer(\n",
    "        learning_rate_input).minimize(cross_entropy_mean)\n",
    "  predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "  correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "  confusion_matrix = tf.math.confusion_matrix(labels=ground_truth_input,\n",
    "                                              predictions=predicted_indices,\n",
    "                                              num_classes=label_count)\n",
    "  evaluation_step = tf.reduce_mean(input_tensor=tf.cast(correct_prediction,\n",
    "                                                        tf.float32))\n",
    "  with tf.compat.v1.get_default_graph().name_scope('eval'):\n",
    "    tf.compat.v1.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "    tf.compat.v1.summary.scalar('accuracy', evaluation_step)\n",
    "\n",
    "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "  increment_global_step = tf.compat.v1.assign(global_step, global_step + 1)\n",
    "\n",
    "  saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "\n",
    "  # Merge all the summaries and write them out to /tmp/retrain_logs (by default)\n",
    "  merged_summaries = tf.compat.v1.summary.merge_all(scope='eval')\n",
    "  train_writer = tf.compat.v1.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                                 sess.graph)\n",
    "  validation_writer = tf.compat.v1.summary.FileWriter(\n",
    "      FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "  tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "  start_step = 1\n",
    "\n",
    "  if FLAGS.start_checkpoint:\n",
    "    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\n",
    "    start_step = global_step.eval(session=sess)\n",
    "\n",
    "  tf.compat.v1.logging.info('Training from step: %d ', start_step)\n",
    "\n",
    "  # Save graph.pbtxt.\n",
    "  tf.io.write_graph(sess.graph_def, FLAGS.train_dir,\n",
    "                    FLAGS.model_architecture + '.pbtxt')\n",
    "\n",
    "  # Save list of words.\n",
    "  with gfile.GFile(\n",
    "      os.path.join(FLAGS.train_dir, FLAGS.model_architecture + '_labels.txt'),\n",
    "      'w') as f:\n",
    "    f.write('\\n'.join(audio_processor.words_list))\n",
    "\n",
    "  # Training loop.\n",
    "  training_steps_max = np.sum(training_steps_list)\n",
    "  for training_step in xrange(start_step, training_steps_max + 1):\n",
    "    # Figure out what the current learning rate is.\n",
    "    training_steps_sum = 0\n",
    "    for i in range(len(training_steps_list)):\n",
    "      training_steps_sum += training_steps_list[i]\n",
    "      if training_step <= training_steps_sum:\n",
    "        learning_rate_value = learning_rates_list[i]\n",
    "        break\n",
    "    # Pull the audio samples we'll use for training.\n",
    "    train_fingerprints, train_ground_truth = audio_processor.get_data(\n",
    "        FLAGS.batch_size, 0, model_settings, FLAGS.background_frequency,\n",
    "        FLAGS.background_volume, time_shift_samples, 'training', sess)\n",
    "    # Run the graph with this batch of training data.\n",
    "    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\n",
    "        [\n",
    "            merged_summaries,\n",
    "            evaluation_step,\n",
    "            cross_entropy_mean,\n",
    "            train_step,\n",
    "            increment_global_step,\n",
    "        ],\n",
    "        feed_dict={\n",
    "            fingerprint_input: train_fingerprints,\n",
    "            ground_truth_input: train_ground_truth,\n",
    "            learning_rate_input: learning_rate_value,\n",
    "            dropout_prob: 0.5\n",
    "        })\n",
    "    train_writer.add_summary(train_summary, training_step)\n",
    "    tf.compat.v1.logging.info(\n",
    "        'Step #%d: rate %f, accuracy %.1f%%, cross entropy %f' %\n",
    "        (training_step, learning_rate_value, train_accuracy * 100,\n",
    "         cross_entropy_value))\n",
    "    is_last_step = (training_step == training_steps_max)\n",
    "    if (training_step % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "      set_size = audio_processor.set_size('validation')\n",
    "      total_accuracy = 0\n",
    "      total_conf_matrix = None\n",
    "      for i in xrange(0, set_size, FLAGS.batch_size):\n",
    "        validation_fingerprints, validation_ground_truth = (\n",
    "            audio_processor.get_data(FLAGS.batch_size, i, model_settings, 0.0,\n",
    "                                     0.0, 0, 'validation', sess))\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy, conf_matrix = sess.run(\n",
    "            [merged_summaries, evaluation_step, confusion_matrix],\n",
    "            feed_dict={\n",
    "                fingerprint_input: validation_fingerprints,\n",
    "                ground_truth_input: validation_ground_truth,\n",
    "                dropout_prob: 1.0\n",
    "            })\n",
    "        validation_writer.add_summary(validation_summary, training_step)\n",
    "        batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "        total_accuracy += (validation_accuracy * batch_size) / set_size\n",
    "        if total_conf_matrix is None:\n",
    "          total_conf_matrix = conf_matrix\n",
    "        else:\n",
    "          total_conf_matrix += conf_matrix\n",
    "      tf.compat.v1.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "      tf.compat.v1.logging.info('Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "                                (training_step, total_accuracy * 100, set_size))\n",
    "\n",
    "    # Save the model checkpoint periodically.\n",
    "    if (training_step % FLAGS.save_step_interval == 0 or\n",
    "        training_step == training_steps_max):\n",
    "      checkpoint_path = os.path.join(FLAGS.train_dir,\n",
    "                                     FLAGS.model_architecture + '.ckpt')\n",
    "      tf.compat.v1.logging.info('Saving to \"%s-%d\"', checkpoint_path,\n",
    "                                training_step)\n",
    "      saver.save(sess, checkpoint_path, global_step=training_step)\n",
    "\n",
    "  set_size = audio_processor.set_size('testing')\n",
    "  tf.compat.v1.logging.info('set_size=%d', set_size)\n",
    "  total_accuracy = 0\n",
    "  total_conf_matrix = None\n",
    "  for i in xrange(0, set_size, FLAGS.batch_size):\n",
    "    test_fingerprints, test_ground_truth = audio_processor.get_data(\n",
    "        FLAGS.batch_size, i, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "    test_accuracy, conf_matrix = sess.run(\n",
    "        [evaluation_step, confusion_matrix],\n",
    "        feed_dict={\n",
    "            fingerprint_input: test_fingerprints,\n",
    "            ground_truth_input: test_ground_truth,\n",
    "            dropout_prob: 1.0\n",
    "        })\n",
    "    batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "    total_accuracy += (test_accuracy * batch_size) / set_size\n",
    "    if total_conf_matrix is None:\n",
    "      total_conf_matrix = conf_matrix\n",
    "    else:\n",
    "      total_conf_matrix += conf_matrix\n",
    "  tf.compat.v1.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "  tf.compat.v1.logging.info('Final test accuracy = %.1f%% (N=%d)' %\n",
    "                            (total_accuracy * 100, set_size))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--data_url',\n",
    "      type=str,\n",
    "      # pylint: disable=line-too-long\n",
    "      default='https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
    "      # pylint: enable=line-too-long\n",
    "      help='Location of speech training data archive on the web.')\n",
    "  parser.add_argument(\n",
    "      '--data_dir',\n",
    "      type=str,\n",
    "      default='/tmp/speech_dataset/',\n",
    "      help=\"\"\"\\\n",
    "      Where to download the speech training data to.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--background_volume',\n",
    "      type=float,\n",
    "      default=0.1,\n",
    "      help=\"\"\"\\\n",
    "      How loud the background noise should be, between 0 and 1.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--background_frequency',\n",
    "      type=float,\n",
    "      default=0.8,\n",
    "      help=\"\"\"\\\n",
    "      How many of the training samples have background noise mixed in.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--silence_percentage',\n",
    "      type=float,\n",
    "      default=10.0,\n",
    "      help=\"\"\"\\\n",
    "      How much of the training data should be silence.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--unknown_percentage',\n",
    "      type=float,\n",
    "      default=10.0,\n",
    "      help=\"\"\"\\\n",
    "      How much of the training data should be unknown words.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--time_shift_ms',\n",
    "      type=float,\n",
    "      default=100.0,\n",
    "      help=\"\"\"\\\n",
    "      Range to randomly shift the training audio by in time.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--testing_percentage',\n",
    "      type=int,\n",
    "      default=10,\n",
    "      help='What percentage of wavs to use as a test set.')\n",
    "  parser.add_argument(\n",
    "      '--validation_percentage',\n",
    "      type=int,\n",
    "      default=10,\n",
    "      help='What percentage of wavs to use as a validation set.')\n",
    "  parser.add_argument(\n",
    "      '--sample_rate',\n",
    "      type=int,\n",
    "      default=16000,\n",
    "      help='Expected sample rate of the wavs',)\n",
    "  parser.add_argument(\n",
    "      '--clip_duration_ms',\n",
    "      type=int,\n",
    "      default=1000,\n",
    "      help='Expected duration in milliseconds of the wavs',)\n",
    "  parser.add_argument(\n",
    "      '--window_size_ms',\n",
    "      type=float,\n",
    "      default=30.0,\n",
    "      help='How long each spectrogram timeslice is.',)\n",
    "  parser.add_argument(\n",
    "      '--window_stride_ms',\n",
    "      type=float,\n",
    "      default=10.0,\n",
    "      help='How far to move in time between spectogram timeslices.',)\n",
    "  parser.add_argument(\n",
    "      '--feature_bin_count',\n",
    "      type=int,\n",
    "      default=40,\n",
    "      help='How many bins to use for the MFCC fingerprint',\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--how_many_training_steps',\n",
    "      type=str,\n",
    "      #default='15000,3000',\n",
    "      default='900,700',\n",
    "      help='How many training loops to run',)\n",
    "  parser.add_argument(\n",
    "      '--eval_step_interval',\n",
    "      type=int,\n",
    "      default=400,\n",
    "      help='How often to evaluate the training results.')\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=str,\n",
    "      default='0.001,0.0001',\n",
    "      help='How large a learning rate to use when training.')\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=10,\n",
    "      help='How many items to train with at once',)\n",
    "  parser.add_argument(\n",
    "      '--summaries_dir',\n",
    "      type=str,\n",
    "      default='/tmp/retrain_logs',\n",
    "      help='Where to save summary logs for TensorBoard.')\n",
    "  parser.add_argument(\n",
    "      '--wanted_words',\n",
    "      type=str,\n",
    "      default='yes,no,up,down,left,right,on,off,stop,go',\n",
    "      help='Words to use (others will be added to an unknown label)',)\n",
    "  parser.add_argument(\n",
    "      '--train_dir',\n",
    "      type=str,\n",
    "      default='/tmp/speech_commands_train',\n",
    "      help='Directory to write event logs and checkpoint.')\n",
    "  parser.add_argument(\n",
    "      '--save_step_interval',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Save model checkpoint every save_steps.')\n",
    "  parser.add_argument(\n",
    "      '--start_checkpoint',\n",
    "      type=str,\n",
    "      default='',\n",
    "      help='If specified, restore this pretrained model before any training.')\n",
    "  parser.add_argument(\n",
    "      '--model_architecture',\n",
    "      type=str,\n",
    "      default='conv',\n",
    "      help='What model architecture to use')\n",
    "  parser.add_argument(\n",
    "      '--check_nans',\n",
    "      type=bool,\n",
    "      default=False,\n",
    "      help='Whether to check for invalid numbers during processing')\n",
    "  parser.add_argument(\n",
    "      '--quantize',\n",
    "      type=bool,\n",
    "      default=False,\n",
    "      help='Whether to train the model for eight-bit deployment')\n",
    "  parser.add_argument(\n",
    "      '--preprocess',\n",
    "      type=str,\n",
    "      default='mfcc',\n",
    "      help='Spectrogram processing mode. Can be \"mfcc\", \"average\", or \"micro\"')\n",
    "\n",
    "  # Function used to parse --verbosity argument\n",
    "  def verbosity_arg(value):\n",
    "    \"\"\"Parses verbosity argument.\n",
    "\n",
    "    Args:\n",
    "      value: A member of tf.logging.\n",
    "    Raises:\n",
    "      ArgumentTypeError: Not an expected value.\n",
    "    \"\"\"\n",
    "    value = value.upper()\n",
    "    if value == 'INFO':\n",
    "      return tf.compat.v1.logging.INFO\n",
    "    elif value == 'DEBUG':\n",
    "      return tf.compat.v1.logging.DEBUG\n",
    "    elif value == 'ERROR':\n",
    "      return tf.compat.v1.logging.ERROR\n",
    "    elif value == 'FATAL':\n",
    "      return tf.compat.v1.logging.FATAL\n",
    "    elif value == 'WARN':\n",
    "      return tf.compat.v1.logging.WARN\n",
    "    else:\n",
    "      raise argparse.ArgumentTypeError('Not an expected value')\n",
    "  parser.add_argument(\n",
    "      '--verbosity',\n",
    "      type=verbosity_arg,\n",
    "      default=tf.compat.v1.logging.INFO,\n",
    "      help='Log verbosity. Can be \"INFO\", \"DEBUG\", \"ERROR\", \"FATAL\", or \"WARN\"')\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1f5864b97098>, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1f5864b97098>\"\u001b[0;36m, line \u001b[0;32m92\u001b[0m\n\u001b[0;31m    input_checkpoint=/media/tegwyn/Xavier_SD/TensorFlow_speech_backups/speech_commands_train_03/conv.ckpt-8100.data-00000-of-00001\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "r\"\"\"Converts checkpoint variables into Const ops in a standalone GraphDef file.\n",
    "\n",
    "This script is designed to take a GraphDef proto, a SaverDef proto, and a set of\n",
    "variable values stored in a checkpoint file, and output a GraphDef with all of\n",
    "the variable ops converted into const ops containing the values of the\n",
    "variables.\n",
    "\n",
    "It's useful to do this when we need to load a single file in C++, especially in\n",
    "environments like mobile or embedded where we may not have access to the\n",
    "RestoreTensor ops and file loading calls that they rely on.\n",
    "\n",
    "An example of command-line usage is:\n",
    "bazel build tensorflow/python/tools:freeze_graph && \\\n",
    "bazel-bin/tensorflow/python/tools/freeze_graph \\\n",
    "--input_graph=some_graph_def.pb \\\n",
    "--input_checkpoint=model.ckpt-8361242 \\\n",
    "--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\n",
    "\n",
    "\n",
    "cd /home/pi/tensorflow/tensorflow_master/tensorflow/python/tools/\n",
    "\n",
    "cd /home/pi/Desktop/TensorFlow_speech_backups/speech_commands_train_03/ &&\n",
    "a7ce2527-ee1b-4edf-a121-1590932e0c66\n",
    "\n",
    "$ s u\n",
    "$ tar zxvf jdk-11.0.4_linux-x64_bin.tar.gz\n",
    "sudo apt-get install rpm\n",
    "$ sudo rpm -ivh jdk-11.interim.update.patch_linux-x64_bin.rpm\n",
    "sudo alien jdk-11.0.4_linux-x64_bin.rpm -d\n",
    "sudo alien filename.rpm\n",
    "\n",
    "To set JAVA_HOME, do one of the following:\n",
    "For Korn and bash shells, run the following commands:\n",
    "export JAVA_HOME=/home/pi/JDK/jdk-11.0.4/bin\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export JAVA_HOME=/path/to/java/version/you/want\n",
    "/usr/lib/jvm/java-11-openjdk-armhf/bin/java\n",
    "\n",
    "sudo ./compile.sh^\n",
    "./compile.sh compile /home/pi/bazel/compile.sh\n",
    "\n",
    "\n",
    "bazel build home/pi/tensorflow/tensorflow_master/tensorflow/python/tools:freeze_graph && \\\n",
    "bazel-home/pi/tensorflow/tensorflow_master/tensorflow/python/tools/freeze_graph \\\n",
    "--input_graph=some_graph_def.pb \\\n",
    "--input_checkpoint=conv.ckpt-8100.data-00000-of-00001 \\\n",
    "--output_graph=/home/pi/Desktop/TensorFlow_speech_backups/frozen_graph.pb --output_node_names=softmax\n",
    "\n",
    "You can also look at freeze_graph_test.py for an example of how to use it.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "from tensorflow.core.protobuf.meta_graph_pb2 import MetaGraphDef\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from tensorflow.python.client import session\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import importer\n",
    "from tensorflow.python.platform import app\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.saved_model import loader\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.python.tools import saved_model_utils\n",
    "from tensorflow.python.training import checkpoint_management\n",
    "from tensorflow.python.training import saver as saver_lib\n",
    "\n",
    "# input_checkpoint=/media/tegwyn/Xavier_SD/TensorFlow_speech_backups/speech_commands_train_03/conv.ckpt-8100.data-00000-of-00001\n",
    "output_graph='/tmp/frozen_graph.pb'\n",
    "output_node_names='softmax'\n",
    "\n",
    "def _has_no_variables(sess):\n",
    "  \"\"\"Determines if the graph has any variables.\n",
    "\n",
    "  Args:\n",
    "    sess: TensorFlow Session.\n",
    "\n",
    "  Returns:\n",
    "    Bool.\n",
    "  \"\"\"\n",
    "  for op in sess.graph.get_operations():\n",
    "    if op.type.startswith(\"Variable\") or op.type.endswith(\"VariableOp\"):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "\n",
    "def freeze_graph_with_def_protos(input_graph_def,\n",
    "                                 input_saver_def,\n",
    "                                 input_checkpoint,\n",
    "                                 output_node_names,\n",
    "                                 restore_op_name,\n",
    "                                 filename_tensor_name,\n",
    "                                 output_graph,\n",
    "                                 clear_devices,\n",
    "                                 initializer_nodes,\n",
    "                                 variable_names_whitelist=\"\",\n",
    "                                 variable_names_blacklist=\"\",\n",
    "                                 input_meta_graph_def=None,\n",
    "                                 input_saved_model_dir=None,\n",
    "                                 saved_model_tags=None,\n",
    "                                 checkpoint_version=saver_pb2.SaverDef.V2):\n",
    "  \"\"\"Converts all variables in a graph and checkpoint into constants.\n",
    "\n",
    "  Args:\n",
    "    input_graph_def: A `GraphDef`.\n",
    "    input_saver_def: A `SaverDef` (optional).\n",
    "    input_checkpoint: The prefix of a V1 or V2 checkpoint, with V2 taking\n",
    "      priority.  Typically the result of `Saver.save()` or that of\n",
    "      `tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or\n",
    "      V1/V2.\n",
    "    output_node_names: The name(s) of the output nodes, comma separated.\n",
    "    restore_op_name: Unused.\n",
    "    filename_tensor_name: Unused.\n",
    "    output_graph: String where to write the frozen `GraphDef`.\n",
    "    clear_devices: A Bool whether to remove device specifications.\n",
    "    initializer_nodes: Comma separated string of initializer nodes to run before\n",
    "                       freezing.\n",
    "    variable_names_whitelist: The set of variable names to convert (optional, by\n",
    "                              default, all variables are converted).\n",
    "    variable_names_blacklist: The set of variable names to omit converting\n",
    "                              to constants (optional).\n",
    "    input_meta_graph_def: A `MetaGraphDef` (optional),\n",
    "    input_saved_model_dir: Path to the dir with TensorFlow 'SavedModel' file\n",
    "                           and variables (optional).\n",
    "    saved_model_tags: Group of comma separated tag(s) of the MetaGraphDef to\n",
    "                      load, in string format (optional).\n",
    "    checkpoint_version: Tensorflow variable file format (saver_pb2.SaverDef.V1\n",
    "                        or saver_pb2.SaverDef.V2)\n",
    "\n",
    "  Returns:\n",
    "    Location of the output_graph_def.\n",
    "  \"\"\"\n",
    "  del restore_op_name, filename_tensor_name  # Unused by updated loading code.\n",
    "\n",
    "  # 'input_checkpoint' may be a prefix if we're using Saver V2 format\n",
    "  if (not input_saved_model_dir and\n",
    "      not checkpoint_management.checkpoint_exists(input_checkpoint)):\n",
    "    raise ValueError(\"Input checkpoint '\" + input_checkpoint +\n",
    "                     \"' doesn't exist!\")\n",
    "\n",
    "  if not output_node_names:\n",
    "    raise ValueError(\n",
    "        \"You need to supply the name of a node to --output_node_names.\")\n",
    "\n",
    "  # Remove all the explicit device specifications for this node. This helps to\n",
    "  # make the graph more portable.\n",
    "  if clear_devices:\n",
    "    if input_meta_graph_def:\n",
    "      for node in input_meta_graph_def.graph_def.node:\n",
    "        node.device = \"\"\n",
    "    elif input_graph_def:\n",
    "      for node in input_graph_def.node:\n",
    "        node.device = \"\"\n",
    "\n",
    "  if input_graph_def:\n",
    "    _ = importer.import_graph_def(input_graph_def, name=\"\")\n",
    "  with session.Session() as sess:\n",
    "    if input_saver_def:\n",
    "      saver = saver_lib.Saver(\n",
    "          saver_def=input_saver_def, write_version=checkpoint_version)\n",
    "      saver.restore(sess, input_checkpoint)\n",
    "    elif input_meta_graph_def:\n",
    "      restorer = saver_lib.import_meta_graph(\n",
    "          input_meta_graph_def, clear_devices=True)\n",
    "      restorer.restore(sess, input_checkpoint)\n",
    "      if initializer_nodes:\n",
    "        sess.run(initializer_nodes.replace(\" \", \"\").split(\",\"))\n",
    "    elif input_saved_model_dir:\n",
    "      if saved_model_tags is None:\n",
    "        saved_model_tags = []\n",
    "      loader.load(sess, saved_model_tags, input_saved_model_dir)\n",
    "    else:\n",
    "      var_list = {}\n",
    "      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n",
    "      var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "\n",
    "      # List of all partition variables. Because the condition is heuristic\n",
    "      # based, the list could include false positives.\n",
    "      all_parition_variable_names = [\n",
    "          tensor.name.split(\":\")[0]\n",
    "          for op in sess.graph.get_operations()\n",
    "          for tensor in op.values()\n",
    "          if re.search(r\"/part_\\d+/\", tensor.name)\n",
    "      ]\n",
    "      has_partition_var = False\n",
    "\n",
    "      for key in var_to_shape_map:\n",
    "        try:\n",
    "          tensor = sess.graph.get_tensor_by_name(key + \":0\")\n",
    "          if any(key in name for name in all_parition_variable_names):\n",
    "            has_partition_var = True\n",
    "        except KeyError:\n",
    "          # This tensor doesn't exist in the graph (for example it's\n",
    "          # 'global_step' or a similar housekeeping element) so skip it.\n",
    "          continue\n",
    "        var_list[key] = tensor\n",
    "\n",
    "      try:\n",
    "        saver = saver_lib.Saver(\n",
    "            var_list=var_list, write_version=checkpoint_version)\n",
    "      except TypeError as e:\n",
    "        # `var_list` is required to be a map of variable names to Variable\n",
    "        # tensors. Partition variables are Identity tensors that cannot be\n",
    "        # handled by Saver.\n",
    "        if has_partition_var:\n",
    "          raise ValueError(\n",
    "              \"Models containing partition variables cannot be converted \"\n",
    "              \"from checkpoint files. Please pass in a SavedModel using \"\n",
    "              \"the flag --input_saved_model_dir.\")\n",
    "        # Models that have been frozen previously do not contain Variables.\n",
    "        elif _has_no_variables(sess):\n",
    "          raise ValueError(\n",
    "              \"No variables were found in this model. It is likely the model \"\n",
    "              \"was frozen previously. You cannot freeze a graph twice.\")\n",
    "          return 0\n",
    "        else:\n",
    "          raise e\n",
    "\n",
    "      saver.restore(sess, input_checkpoint)\n",
    "      if initializer_nodes:\n",
    "        sess.run(initializer_nodes.replace(\" \", \"\").split(\",\"))\n",
    "\n",
    "    variable_names_whitelist = (\n",
    "        variable_names_whitelist.replace(\" \", \"\").split(\",\")\n",
    "        if variable_names_whitelist else None)\n",
    "    variable_names_blacklist = (\n",
    "        variable_names_blacklist.replace(\" \", \"\").split(\",\")\n",
    "        if variable_names_blacklist else None)\n",
    "\n",
    "    if input_meta_graph_def:\n",
    "      output_graph_def = graph_util.convert_variables_to_constants(\n",
    "          sess,\n",
    "          input_meta_graph_def.graph_def,\n",
    "          output_node_names.replace(\" \", \"\").split(\",\"),\n",
    "          variable_names_whitelist=variable_names_whitelist,\n",
    "          variable_names_blacklist=variable_names_blacklist)\n",
    "    else:\n",
    "      output_graph_def = graph_util.convert_variables_to_constants(\n",
    "          sess,\n",
    "          input_graph_def,\n",
    "          output_node_names.replace(\" \", \"\").split(\",\"),\n",
    "          variable_names_whitelist=variable_names_whitelist,\n",
    "          variable_names_blacklist=variable_names_blacklist)\n",
    "\n",
    "  # Write GraphDef to file if output path has been given.\n",
    "  if output_graph:\n",
    "    with gfile.GFile(output_graph, \"wb\") as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "  return output_graph_def\n",
    "\n",
    "\n",
    "def _parse_input_graph_proto(input_graph, input_binary):\n",
    "  \"\"\"Parses input tensorflow graph into GraphDef proto.\"\"\"\n",
    "  if not gfile.Exists(input_graph):\n",
    "    raise IOError(\"Input graph file '\" + input_graph + \"' does not exist!\")\n",
    "  input_graph_def = graph_pb2.GraphDef()\n",
    "  mode = \"rb\" if input_binary else \"r\"\n",
    "  with gfile.GFile(input_graph, mode) as f:\n",
    "    if input_binary:\n",
    "      input_graph_def.ParseFromString(f.read())\n",
    "    else:\n",
    "      text_format.Merge(f.read(), input_graph_def)\n",
    "  return input_graph_def\n",
    "\n",
    "\n",
    "def _parse_input_meta_graph_proto(input_graph, input_binary):\n",
    "  \"\"\"Parses input tensorflow graph into MetaGraphDef proto.\"\"\"\n",
    "  if not gfile.Exists(input_graph):\n",
    "    raise IOError(\"Input meta graph file '\" + input_graph + \"' does not exist!\")\n",
    "  input_meta_graph_def = MetaGraphDef()\n",
    "  mode = \"rb\" if input_binary else \"r\"\n",
    "  with gfile.GFile(input_graph, mode) as f:\n",
    "    if input_binary:\n",
    "      input_meta_graph_def.ParseFromString(f.read())\n",
    "    else:\n",
    "      text_format.Merge(f.read(), input_meta_graph_def)\n",
    "  print(\"Loaded meta graph file '\" + input_graph)\n",
    "  return input_meta_graph_def\n",
    "\n",
    "\n",
    "def _parse_input_saver_proto(input_saver, input_binary):\n",
    "  \"\"\"Parses input tensorflow Saver into SaverDef proto.\"\"\"\n",
    "  if not gfile.Exists(input_saver):\n",
    "    raise IOError(\"Input saver file '\" + input_saver + \"' does not exist!\")\n",
    "  mode = \"rb\" if input_binary else \"r\"\n",
    "  with gfile.GFile(input_saver, mode) as f:\n",
    "    saver_def = saver_pb2.SaverDef()\n",
    "    if input_binary:\n",
    "      saver_def.ParseFromString(f.read())\n",
    "    else:\n",
    "      text_format.Merge(f.read(), saver_def)\n",
    "  return saver_def\n",
    "\n",
    "\n",
    "def freeze_graph(input_graph,\n",
    "                 input_saver,\n",
    "                 input_binary,\n",
    "                 input_checkpoint,\n",
    "                 output_node_names,\n",
    "                 restore_op_name,\n",
    "                 filename_tensor_name,\n",
    "                 output_graph,\n",
    "                 clear_devices,\n",
    "                 initializer_nodes,\n",
    "                 variable_names_whitelist=\"\",\n",
    "                 variable_names_blacklist=\"\",\n",
    "                 input_meta_graph=None,\n",
    "                 input_saved_model_dir=None,\n",
    "                 saved_model_tags=tag_constants.SERVING,\n",
    "                 checkpoint_version=saver_pb2.SaverDef.V2):\n",
    "  \"\"\"Converts all variables in a graph and checkpoint into constants.\n",
    "\n",
    "  Args:\n",
    "    input_graph: A `GraphDef` file to load.\n",
    "    input_saver: A TensorFlow Saver file.\n",
    "    input_binary: A Bool. True means input_graph is .pb, False indicates .pbtxt.\n",
    "    input_checkpoint: The prefix of a V1 or V2 checkpoint, with V2 taking\n",
    "      priority.  Typically the result of `Saver.save()` or that of\n",
    "      `tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or\n",
    "      V1/V2.\n",
    "    output_node_names: The name(s) of the output nodes, comma separated.\n",
    "    restore_op_name: Unused.\n",
    "    filename_tensor_name: Unused.\n",
    "    output_graph: String where to write the frozen `GraphDef`.\n",
    "    clear_devices: A Bool whether to remove device specifications.\n",
    "    initializer_nodes: Comma separated list of initializer nodes to run before\n",
    "                       freezing.\n",
    "    variable_names_whitelist: The set of variable names to convert (optional, by\n",
    "                              default, all variables are converted),\n",
    "    variable_names_blacklist: The set of variable names to omit converting\n",
    "                              to constants (optional).\n",
    "    input_meta_graph: A `MetaGraphDef` file to load (optional).\n",
    "    input_saved_model_dir: Path to the dir with TensorFlow 'SavedModel' file and\n",
    "                           variables (optional).\n",
    "    saved_model_tags: Group of comma separated tag(s) of the MetaGraphDef to\n",
    "                      load, in string format.\n",
    "    checkpoint_version: Tensorflow variable file format (saver_pb2.SaverDef.V1\n",
    "                        or saver_pb2.SaverDef.V2).\n",
    "  Returns:\n",
    "    String that is the location of frozen GraphDef.\n",
    "  \"\"\"\n",
    "  input_graph_def = None\n",
    "  if input_saved_model_dir:\n",
    "    input_graph_def = saved_model_utils.get_meta_graph_def(\n",
    "        input_saved_model_dir, saved_model_tags).graph_def\n",
    "  elif input_graph:\n",
    "    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\n",
    "  input_meta_graph_def = None\n",
    "  if input_meta_graph:\n",
    "    input_meta_graph_def = _parse_input_meta_graph_proto(\n",
    "        input_meta_graph, input_binary)\n",
    "  input_saver_def = None\n",
    "  if input_saver:\n",
    "    input_saver_def = _parse_input_saver_proto(input_saver, input_binary)\n",
    "  return freeze_graph_with_def_protos(\n",
    "      input_graph_def,\n",
    "      input_saver_def,\n",
    "      input_checkpoint,\n",
    "      output_node_names,\n",
    "      restore_op_name,\n",
    "      filename_tensor_name,\n",
    "      output_graph,\n",
    "      clear_devices,\n",
    "      initializer_nodes,\n",
    "      variable_names_whitelist,\n",
    "      variable_names_blacklist,\n",
    "      input_meta_graph_def,\n",
    "      input_saved_model_dir,\n",
    "      saved_model_tags.replace(\" \", \"\").split(\",\"),\n",
    "      checkpoint_version=checkpoint_version)\n",
    "\n",
    "\n",
    "def main(unused_args, flags):\n",
    "  if flags.checkpoint_version == 1:\n",
    "    checkpoint_version = saver_pb2.SaverDef.V1\n",
    "  elif flags.checkpoint_version == 2:\n",
    "    checkpoint_version = saver_pb2.SaverDef.V2\n",
    "  else:\n",
    "    raise ValueError(\"Invalid checkpoint version (must be '1' or '2'): %d\" %\n",
    "                     flags.checkpoint_version)\n",
    "  freeze_graph(flags.input_graph, flags.input_saver, flags.input_binary,\n",
    "               flags.input_checkpoint, flags.output_node_names,\n",
    "               flags.restore_op_name, flags.filename_tensor_name,\n",
    "               flags.output_graph, flags.clear_devices, flags.initializer_nodes,\n",
    "               flags.variable_names_whitelist, flags.variable_names_blacklist,\n",
    "               flags.input_meta_graph, flags.input_saved_model_dir,\n",
    "               flags.saved_model_tags, checkpoint_version)\n",
    "\n",
    "\n",
    "def run_main():\n",
    "  \"\"\"Main function of freeze_graph.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  parser.add_argument(\n",
    "      \"--input_graph\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"TensorFlow \\'GraphDef\\' file to load.\")\n",
    "  parser.add_argument(\n",
    "      \"--input_saver\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"TensorFlow saver file to load.\")\n",
    "  parser.add_argument(\n",
    "      \"--input_checkpoint\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"TensorFlow variables file to load.\")\n",
    "  parser.add_argument(\n",
    "      \"--checkpoint_version\",\n",
    "      type=int,\n",
    "      default=2,\n",
    "      help=\"Tensorflow variable file format\")\n",
    "  parser.add_argument(\n",
    "      \"--output_graph\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Output \\'GraphDef\\' file name.\")\n",
    "  parser.add_argument(\n",
    "      \"--input_binary\",\n",
    "      nargs=\"?\",\n",
    "      const=True,\n",
    "      type=\"bool\",\n",
    "      default=False,\n",
    "      help=\"Whether the input files are in binary format.\")\n",
    "  parser.add_argument(\n",
    "      \"--output_node_names\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"The name of the output nodes, comma separated.\")\n",
    "  parser.add_argument(\n",
    "      \"--restore_op_name\",\n",
    "      type=str,\n",
    "      default=\"save/restore_all\",\n",
    "      help=\"\"\"\\\n",
    "      The name of the master restore operator. Deprecated, unused by updated \\\n",
    "      loading code.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--filename_tensor_name\",\n",
    "      type=str,\n",
    "      default=\"save/Const:0\",\n",
    "      help=\"\"\"\\\n",
    "      The name of the tensor holding the save path. Deprecated, unused by \\\n",
    "      updated loading code.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--clear_devices\",\n",
    "      nargs=\"?\",\n",
    "      const=True,\n",
    "      type=\"bool\",\n",
    "      default=True,\n",
    "      help=\"Whether to remove device specifications.\")\n",
    "  parser.add_argument(\n",
    "      \"--initializer_nodes\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma separated list of initializer nodes to run before freezing.\")\n",
    "  parser.add_argument(\n",
    "      \"--variable_names_whitelist\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"\"\"\\\n",
    "      Comma separated list of variables to convert to constants. If specified, \\\n",
    "      only those variables will be converted to constants.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--variable_names_blacklist\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"\"\"\\\n",
    "      Comma separated list of variables to skip converting to constants.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--input_meta_graph\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"TensorFlow \\'MetaGraphDef\\' file to load.\")\n",
    "  parser.add_argument(\n",
    "      \"--input_saved_model_dir\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the dir with TensorFlow \\'SavedModel\\' file and variables.\")\n",
    "  parser.add_argument(\n",
    "      \"--saved_model_tags\",\n",
    "      type=str,\n",
    "      default=\"serve\",\n",
    "      help=\"\"\"\\\n",
    "      Group of tag(s) of the MetaGraphDef to load, in string format,\\\n",
    "      separated by \\',\\'. For tag-set contains multiple tags, all tags \\\n",
    "      must be passed in.\\\n",
    "      \"\"\")\n",
    "  flags, unparsed = parser.parse_known_args()\n",
    "\n",
    "  my_main = lambda unused_args: main(unused_args, flags)\n",
    "  app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  run_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
